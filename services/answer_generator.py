import os
from openai import OpenAI
from fastapi import HTTPException
from services.graph import retrieve_relevant_chunks_from_db

# Load OpenAI API key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)


def generate_answer(doc_id: str, query: str):
    """
    Generate an answer based on document content and the given query by first retrieving relevant chunks
    from the database and then passing them to OpenAI's GPT-4 model for a completion.

    Steps involved:
    1. Retrieve relevant document chunks from the database using the `retrieve_relevant_chunks_from_db` function.
    2. Construct a context by joining the text of the relevant chunks.
    3. Create a prompt for the GPT-4 model that includes the context and query.
    4. Call the OpenAI API to generate a response based on the provided context and question.
    5. Return the generated answer from the model.

    Parameters:
    - doc_id (str): The document ID used to query and retrieve related chunks from the database.
    - query (str): The question that the user has, which will be answered based on the document's content.

    Returns:
    - str: The answer generated by the GPT-4 model.
    """
    # Step 1: Retrieve relevant chunks from the database
    try:
        relevant_chunks = retrieve_relevant_chunks_from_db(doc_id, query, top_k=3)
    except HTTPException as e:
        raise HTTPException(status_code=e.status_code, detail=e.detail)

    # Step 2: Prepare the context by joining the relevant chunks' text
    texts = [chunk["text"] for chunk in relevant_chunks]

    # Join the extracted texts into a single string
    context = " ".join(texts)

    # # Step 3: Prepare the prompt for the LLM
    prompt = f"""
    Context: {context}

    Question: {query}

    Answer: Please answer the question based only on the information from the provided context. 
    If the answer is not directly available or the context is not relevant, kindly explain that based on the document, you're unable to answer the question. 
    Additionally, keep a motivational and reflective tone to encourage learning and exploration. 
    If you're unsure, it's okay to acknowledge that and prompt the user to refine their query for more clarity.
    """

    # # Step 4: Make the LLM request using the correct chat completion endpoint
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "You are a supportive learning assistant. Use the following principles in responding to employees:\n"
                "    - Always maintain a friendly and motivational tone. Encourage employees to think critically and explore new ideas.\n"
                "    - Base your responses only on the information provided within the context of the document.\n"
                "    - If the context doesn't provide enough information or is not relevant to the question, inform the user politely that you cannot provide an answer.\n"
                "    - Acknowledge the uncertainty if needed, and encourage users to rephrase their question for better clarity.\n"
                "    - Foster an atmosphere of curiosity, guiding users to discover insights independently where possible.",
            },
            {
                "role": "user",
                "content": query,
            },
            {
                "role": "system",
                "content": prompt,
            },
        ],
        temperature=0.8,
        max_tokens=200,
        top_p=1,
    )

    # Return the generated answer
    return response.choices[0].message
