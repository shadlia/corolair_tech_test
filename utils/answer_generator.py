import os
from openai import OpenAI
from langchain_openai import OpenAI as langchain_openai

from fastapi import HTTPException
from utils.Knowlege_graph import retrieve_relevant_chunks_from_db
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI as LangChainOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.agents import Tool

# Load OpenAI API key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=OPENAI_API_KEY)
# Set up LangChain OpenAI client for agent
llm = langchain_openai(model_name="gpt-3.5-turbo-instruct", api_key=OPENAI_API_KEY)


def generate_answer(doc_id: str, query: str):
    """
    Generate an answer based on document content and the given query by first retrieving relevant chunks
    from the database and then passing them to OpenAI's GPT-4 model for a completion.

    If the generated answer is irrelevant, fall back to an agent to generate an alternative response.

    Parameters:
    - doc_id (str): The document ID used to query and retrieve related chunks from the database.
    - query (str): The question that the user has, which will be answered based on the document's content.

    Returns:
    - dict: A dictionary containing the generated answer and relevance status.
    """
    # Step 1: Retrieve relevant chunks from the database
    try:
        relevant_chunks = retrieve_relevant_chunks_from_db(doc_id, query, top_k=3)
    except HTTPException as e:
        raise HTTPException(status_code=e.status_code, detail=e.detail)

    # Step 2: Prepare the context by joining the relevant chunks' text
    texts = [chunk["text"] for chunk in relevant_chunks]
    context = " ".join(texts)

    # Step 3: Prepare the prompt for the LLM
    prompt = f"""
        Context: {context}

        Question: {query}

        Answer: Please answer the question based only on the information from the provided context. 
        If the answer is not directly available or the context is not relevant, kindly explain that based on the document, you're unable to answer the question.
        If you cannot provide a relevant answer, mark the response with "irrelevant: true" and kindly explain that based on the document, you're unable to answer the question. 
        Additionally, please structure the response in JSON format as follows:

        {{
            "content": "<your generated text>",
            "relevant": <true or false>
        }}

        If the answer is relevant, set "relevant" to true, otherwise set it to false. 
        Keep the tone motivational, reflective, and encourage learning and exploration.
    """

    # Step 4: Make the LLM request using the correct chat completion endpoint
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "You are a supportive learning assistant. Use the following principles in responding to employees:\n"
                "    - Always maintain a friendly and motivational tone. Encourage employees to think critically and explore new ideas.\n"
                "    - Base your responses only on the information provided within the context of the document.\n"
                "    - If the context doesn't provide enough information or is not relevant to the question, inform the user politely that you cannot provide an answer.\n"
                "    - Acknowledge the uncertainty if needed, and encourage users to rephrase their question for better clarity.\n"
                "    - Foster an atmosphere of curiosity, guiding users to discover insights independently where possible.",
            },
            {
                "role": "user",
                "content": query,
            },
            {
                "role": "system",
                "content": prompt,
            },
        ],
        temperature=0.8,
        max_tokens=200,
        top_p=1,
    )

    # Extract the answer and check if it's relevant
    answer_content = response.choices[0].message.content

    # Check if the answer is relevant (e.g., empty or irrelevant content)
    if "irrelevant" in answer_content.lower() or not answer_content.strip():
        return {"content": answer_content, "irrelevant": True}

    return {"content": answer_content, "irrelevant": False}


def agent_start(query: str) -> str:
    """
    Fallback agent function that can generate alternative answers when the document-based answer is irrelevant.

    Parameters:
    - query (str): The query from the user that the agent will handle if no relevant answer is found.

    Returns:
    - str: The alternative response generated by the agent.
    """
    return f"Sorry, I couldn't find a relevant answer from the document. Here's an alternative answer to your question: {query}"


# Define a tool for the agent (a simple fallback tool)
fallback_tool = Tool(
    name="FallbackAgent",
    func=agent_start,
    description="This tool provides alternative answers when the original answer is deemed irrelevant.",
)

# Set up LangChain Agent
agent = initialize_agent(
    tools=[fallback_tool],
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    llm=llm,
    verbose=True,
)
